alg: IPLearnSAC
alg_kwargs:
  random_steps: 10000
  reward_freq: 5000
  max_feedback: 8000
  init_feedback_size: 40
  chi2_coeff: 0.5
  chi2_replay_weight: 0.5
  policy_replay_weight: 0.5
  use_min_target: False
  use_soft_target: False

optim: Adam
optim_kwargs:
  lr: 0.0003

network: ActorCriticPolicy
network_kwargs:
  actor_class: DiagonalGaussianMLPActor
  actor_kwargs:
    hidden_layers: [256, 256, 256]
    log_std_bounds: [-5, 2]
    ortho_init: true
  critic_class: ContinuousMLPCritic
  critic_kwargs:
    hidden_layers: [256, 256, 256]
    ensemble_size: 3
    ortho_init: true

env: mw_door-close-v2

dataset: ReplayAndFeedbackBuffer
dataset_kwargs:
  replay_class: ReplayBuffer
  replay_kwargs:
    distributed: False
    capacity: 1000000
    fetch_every: 500
    batch_size: 256
  feedback_class: PairwiseComparisonDataset
  feedback_kwargs:
    capacity: 8000 # Set to max feedback
    batch_size: 32
    segment_size: 25
    subsample_size: 13
  discount: 0.99

processor: null

trainer_kwargs: # Arguments given to Algorithm.train
  total_steps: 1000000 # The total number of steps to train
  log_freq: 500 # How often to log values
  eval_freq: 10000 # How often to run evals
  eval_fn: eval_policy
  eval_kwargs:
    num_ep: 25 # Number of enviornment episodes to run for evaluation
  loss_metric: reward # The validation metric that determines when to save the "best_checkpoint"
  train_dataloader_kwargs:
    num_workers: 0
    batch_size: null
    collate_fn: null
  profile_freq: 500

checkpoint: null
seed: null
